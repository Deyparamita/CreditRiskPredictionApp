# -*- coding: utf-8 -*-
"""PredictiongCreditRisk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jXuwbJiExW38HivqUwCbPmSBdWjgmYwU

# Credit Risk Prediction
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
import joblib

df = pd.read_csv('german_credit_data.csv')

df.head()

print(df.info())

print(df.describe())

df_processed = df.drop('Unnamed: 0', axis=1)

df_processed

print(df_processed['Checking account'].isnull().sum())
print(df_processed['Saving accounts'].isnull().sum())

df_processed['Checking account'].fillna('no_info', inplace=True)
df_processed['Saving accounts'].fillna('no_info', inplace=True)
df_processed.head()

# Create Target Variable (Risk Column)
df_processed['Risk'] = np.where(
    (df['Credit amount'] > 5000) & (df['Duration'] > 24) & (df['Saving accounts'] == 'little'),
    0,
    1
)

df_processed

# Encode Categorical Columns
label_encoders = {}
for col in df_processed.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df_processed[col] = le.fit_transform(df_processed[col])
    label_encoders[col] = le   # Save encoders for Streamlit later

# Example: Create a feature "Credit per month"
df_processed['Credit_per_month'] = df_processed['Credit amount'] / (df_processed['Duration'] + 1)  # +1 to avoid division by zero

# 3. Splitting Data

X = df_processed.drop('Risk', axis=1)
y = df_processed['Risk']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X.shape, y.shape

# 4. Scaling Features

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Model Training with Random Forest
rfc = RandomForestClassifier(random_state=42)

# Hyperparameter Tuning using GridSearchCV
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')

grid_search.fit(X_train_scaled, y_train)

best_model = grid_search.best_estimator_

best_model

# 6. Model Evaluation

y_pred = best_model.predict(X_test_scaled)

print(classification_report(y_test, y_pred))

accuracy_score(y_test, y_pred)

# Confusion Matrix
plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Feature Importance
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print(feature_importances)

import os
os.makedirs('models', exist_ok=True)

joblib.dump(best_model, 'models/best_model.pkl')
joblib.dump(scaler, 'models/scaler.pkl')
joblib.dump(label_encoders, 'models/label_encoders.pkl')

